{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Analysis Code Report\n",
    "\n",
    "This ipython notebook contains all code run to reproduce the results discussed in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common utility functions\n",
    "\n",
    "The data once loaded is split to 67% training data and 33% training data. This common split of data is used by every technique evaluated here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data loaded\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Read the data\n",
    "originalTrainData = pd.read_csv(os.path.join('data', 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3);\n",
    "[trainData, testData] = train_test_split(originalTrainData, test_size=0.33, random_state=42);\n",
    "\n",
    "extraWordVecData = pd.read_csv(os.path.join('data', 'unlabeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3);\n",
    "\n",
    "print 'Text Data loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the ROC under curve we use the following utility  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotROC(result,algorithm):\n",
    "    # plot the ROC curve and show the area under curve\n",
    "    # TODO : Should we plot this\n",
    "    [fpr,tpr,threshold] = roc_curve(result[\"expected_sentiment\"],result[\"sentiment\"]);\n",
    "    \n",
    "    score = roc_auc_score(result[\"expected_sentiment\"],result[\"sentiment\"]);\n",
    "    print \"ROC under curve for bag of Words:\"\n",
    "    print score ;\n",
    "    print \" Algorithm used :\"+ algorithm;\n",
    "\n",
    "    #store the information in an output file\n",
    "    result.to_csv(os.path.join('logs',algorithm +\".csv\"), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "We use traditional approach of bag of words to classify sentiments. Without taking structure of sentence and word arrangement into consideration, it relies on \"anchor\" words to be able to classify correctly. For the implementation I use one-hot-K encoding for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest (this may take a while)...\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Predicting test labels...\n",
      "\n",
      "ROC under curve for bag of Words:\n",
      "0.845485587173\n",
      " Algorithm used :Bag of Words\n"
     ]
    }
   ],
   "source": [
    "from BagOfWords import BagOfWords\n",
    "plotROC(BagOfWords(trainData, testData), \"Bag of Words\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Approach\n",
    "\n",
    "The dimensionality requirement of one-hot K encoding makes the problem much larger. Also a lot of information of word relations (like man is to woman as king is to queen) is lost in case of traditional one-hot K encoding. As suggested in Kaggle competition, we train the word vector model to reduce dimensionality of the problem. Then we used Average Word vector and K-means classifier for sentiment classification over the tensor for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector model already exists\n"
     ]
    }
   ],
   "source": [
    "from WordVectorModelUtility import LoadWordVectorModel, TrainWordVectorModel\n",
    "from Word2Vec_AverageVectors import AverageWordVector\n",
    "from Word2Vec_BagOfCentroids import BagOfCentroids\n",
    "\n",
    "wordVecModelName = \"ComparisonModelv2\";\n",
    "TrainWordVectorModel(wordVecModelName, originalTrainData, extraWordVecData);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the bag of words approach with much smaller feature size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for training reviews\n",
      "Creating average feature vecs for test reviews\n",
      "Fitting a random forest to labeled training data...\n",
      "ROC under curve for bag of Words:\n",
      "0.843399220404\n",
      " Algorithm used :Word vector with averaging\n"
     ]
    }
   ],
   "source": [
    "plotROC(AverageWordVector(LoadWordVectorModel(wordVecModelName), trainData,testData),\"Word vector with averaging\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same with a slightly different approach to using word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K means\n",
      "Time taken for K Means clustering:  2857.75741005 seconds.\n",
      "\n",
      "Cluster 0\n",
      "[u'publicity', u'paycheck']\n",
      "\n",
      "Cluster 1\n",
      "[u'prototype', u'hannibal']\n",
      "\n",
      "Cluster 2\n",
      "[u'nigh', u'foe', u'consultant', u'embassy', u'challenger', u'ordered', u'renegade', u'commission', u'grants', u'volunteer', u'mercury', u'proposed', u'brass', u'chairman', u'tabloid', u'shipping']\n",
      "\n",
      "Cluster 3\n",
      "[u'basics', u'screamers']\n",
      "\n",
      "Cluster 4\n",
      "[u'passages', u'happenings', u'glimpses', u'scattered']\n",
      "\n",
      "Cluster 5\n",
      "[u'controversy', u'globalization', u'accustomed', u'promotes', u'sentiment', u'evolved', u'clearer']\n",
      "\n",
      "Cluster 6\n",
      "[u'widmark', u'jacob', u'louis', u'dreyfuss', u'hines', u'chamberlain', u'foley', u'cardinal', u'dreyfus', u'crenna', u'farnsworth']\n",
      "\n",
      "Cluster 7\n",
      "[u'exorcism', u'whipping', u'torturing', u'lovemaking', u'stabs', u'organ', u'foxes', u'bursts', u'stall']\n",
      "\n",
      "Cluster 8\n",
      "[u'macho', u'manipulative', u'vulnerable', u'compassionate', u'egotistical', u'neurotic', u'shy', u'smug', u'bratty', u'arrogant', u'naive', u'insecure', u'precocious', u'overbearing', u'confident', u'conflicted', u'effeminate', u'selfish']\n",
      "\n",
      "Cluster 9\n",
      "[u'drawn']\n",
      "Cleaning training reviews\n",
      "Cleaning test reviews\n",
      "Fitting a random forest to labeled training data...\n",
      "ROC under curve for bag of Words:\n",
      "0.840049132468\n",
      " Algorithm used :Word vector with bag of centroids\n"
     ]
    }
   ],
   "source": [
    "plotROC(BagOfCentroids(LoadWordVectorModel(wordVecModelName),trainData,testData),\"Word vector with bag of centroids\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector based classifier\n",
    "Unlike word vector which loses the notion of sentence structure, paragraph vector maintains a notion of sentence structure. Therefore the sequence and sentence structure information is maintained in some way. For example, I am not not interested may be lost in any model not using sentence structure free model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc vector model already exists\n",
      "Doc vector model already exists\n",
      "ROC under curve for bag of Words:\n",
      "0.820424061392\n",
      " Algorithm used :Paragraph Vector Result_dm : \n",
      "ROC under curve for bag of Words:\n",
      "0.87692742823\n",
      " Algorithm used :Paragraph Vector Result_dbow : \n"
     ]
    }
   ],
   "source": [
    "from ParagraphVector import TrainReviewVectorModel, LoadReviewVectorModel\n",
    "from sklearn import svm\n",
    "\n",
    "sentenceVecModelName = \"ReviewComparisonModel\";\n",
    "TrainReviewVectorModel(sentenceVecModelName, originalTrainData, \\\n",
    "                       extraWordVecData);\n",
    "\n",
    "TrainReviewVectorModel(sentenceVecModelName, originalTrainData, \\\n",
    "                       extraWordVecData);\n",
    "\n",
    "def svcClassifier(paragraphVectorModel, trainData, testData, suffix):\n",
    "    # Use linear SVC as it is less than 100k samples 50-75k samples\n",
    "    # REFERENCE: \n",
    "    # http://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "    classifier = svm.SVC();\n",
    "    # Use the vector representation from paragraph vector\n",
    "\n",
    "    classifier.fit(paragraphVectorModel.docvecs[trainData[\"id\"]], \\\n",
    "                   trainData[\"sentiment\"])\n",
    "\n",
    "    result = pd.DataFrame( data=\\\n",
    "                           {\"id\":testData[\"id\"], \\\n",
    "                            \"sentiment\": classifier.predict(paragraphVectorModel.docvecs[testData[\"id\"]]),\\\n",
    "                            \"expected_sentiment\": \\\n",
    "                           testData[\"sentiment\"]} );\n",
    "    plotROC(result, \"Paragraph Vector Result\" + suffix +\" : \");\n",
    "\n",
    "svcClassifier(LoadReviewVectorModel(sentenceVecModelName, True), trainData, testData,\"_dm\");\n",
    "svcClassifier(LoadReviewVectorModel(sentenceVecModelName, False), trainData, testData,\"_dbow\");    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Classifier\n",
    "It took too long on my machine for the training to complete for RNN in iPython. This took a long time. It took even longer to generate the results in iPython. While it took about 6-7 hours when running it as part of Python script, it took 3 days for it to finish in iPython\n",
    "\n",
    "So I switched to python to finish the training and testing by running the RNNClassifier_passage and caching the results in the required formats in log. On generating the PlotROC results from logs, I get the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "Passage_Lib = os.path.join(os.getenv('HOME'),'anaconda2', 'lib','python2.7','site-packages');\n",
    "sys.path.append(Passage_Lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training ...\n",
      "Epoch 0 Seen 16391 samples Avg cost 0.6914 Time elapsed 16697 seconds\n",
      "Epoch 1 Seen 32782 samples Avg cost 0.6775 Time elapsed 50671 seconds\n",
      "Epoch 2 Seen 49173 samples Avg cost 0.6791 Time elapsed 67278 seconds\n",
      "Epoch 3 Seen 65564 samples Avg cost 0.5879 Time elapsed 88099 seconds\n",
      "Epoch 4 Seen 81955 samples Avg cost 0.4913 Time elapsed 104853 seconds\n",
      "Epoch 5 Seen 98346 samples Avg cost 0.3822 Time elapsed 132006 seconds\n",
      "Epoch 6 Seen 114737 samples Avg cost 0.3186 Time elapsed 148636 seconds\n",
      "Epoch 7 Seen 131128 samples Avg cost 0.2729 Time elapsed 169448 seconds\n",
      "Epoch 8 Seen 147519 samples Avg cost 0.2515 Time elapsed 188591 seconds\n",
      "Epoch 9 Seen 163910 samples Avg cost 0.2296 Time elapsed 205305 seconds\n",
      "Prediction error on training set\n",
      "Training Prediction Accuracy 0.934865671642\n",
      "Predicting ...\n",
      "ROC under curve for bag of Words:\n",
      "0.901849520062\n",
      " Algorithm used :RNN with GRU over 10 epochs\n"
     ]
    }
   ],
   "source": [
    "from RNNClassifier_passage import TrainTestRNN\n",
    "\n",
    "plotROC(TrainTestRNN(trainData, testData),\"RNN with GRU over 10 epochs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seens so far Recurrent Neural Network has the best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ensemble Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the results of the various models identified so far--\n",
      "Majority voting results.csv\n",
      "Word vector with averaging.csv\n",
      "Paragraph Vector Resut_dbow : .csv\n",
      "Paragraph Vector Result_dm : .csv\n",
      "Paragraph Vector Result_dbow : .csv\n",
      "Paragraph Vector Resut_dm : .csv\n",
      "RNN with GRU over 10 epochs.csv\n",
      "Bag of Words.csv\n",
      "Paragraph Vector SVM classification.csv\n",
      "Word vector with bag of centroids.csv\n",
      "ROC under curve for bag of Words:\n",
      "0.897815191982\n",
      " Algorithm used :Majority voting results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"Using the results of the various models identified so far--\"\n",
    "for filename in os.listdir('logs'):\n",
    "    if filename.endswith('csv'):\n",
    "        print(filename) ;\n",
    "\n",
    "results = [pd.read_csv(os.path.join('logs',filename)) for filename in os.listdir('logs') if filename.endswith('csv')];\n",
    "\n",
    "sum_result = np.zeros(len(results[0].sentiment))\n",
    "for result in results:\n",
    "    sum_result += np.array(result.sentiment);\n",
    "\n",
    "majority_voting_result = sum_result >=(len(results)*1.0/2)\n",
    "\n",
    "majority_result = pd.DataFrame( data=\\\n",
    "                           {\"id\":results[0][\"id\"], \\\n",
    "                            \"sentiment\": majority_voting_result,\\\n",
    "                            \"expected_sentiment\": \\\n",
    "                           results[0][\"expected_sentiment\"]} );\n",
    "\n",
    "plotROC(majority_result,\"Majority voting results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I tried simple majority based ensemble (which would be same as bag of words). The intuition was that the techniques of boosting or ensemble learning shall be useful to improve the overall accuracy of the system. However, I wasn't able to get highest success with a simple majority based classification technique. \n",
    "\n",
    "From what I have seen in lectures (about boosting & ensemble learning) and what I have learnt from discussion forums in Kaggle, I do believe Ensemble learning if applied correctly should give a better result. For example,if probability of correct classification is 0.7,0.7,0.7. Even with simple majority voting, the probability of error comes to 0.3x0.3x0.3 + 0.3x0.3x0.7x3 = 0.216 which is lower than individual errors.\n",
    "\n",
    "The simpler approach adopted here may have worked better if there was a closer results with the different techniques. A better approach may have been to try to check the results with weighted averaging of ensemble\n",
    "\n",
    "However, since that is currently beyond the scope of original intent of the proposal, I have marked this as a task to pursue separately to avoid scope creep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher dimensions for word vector, paragraph vector\n",
    "Naively increasing the dimensions considered for the problem (i.e. number of words considered)\n",
    "### Better Ensembles\n",
    "More selective and taking into account the accuracy of each model used.\n",
    "\n",
    "### RNN models using word vectors\n",
    "Apart from time taken to train, it shall also capture semantic relations between words.\n",
    "\n",
    "### RNTN based model for sentiment classification\n",
    "Stanford tree parser method has shown promising results. But the setup effort looked expensive enough for an optional exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
